{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# Understanding RustBPETokenizer.train_from_iterator()\n\nThis notebook breaks down the `train_from_iterator()` function step by step to understand how BPE tokenization training works.\n\n## What is BPE (Byte Pair Encoding)?\n\nBPE is a compression algorithm adapted for tokenization:\n1. Start with individual bytes (256 base tokens)\n2. Find the most frequent pair of tokens\n3. Merge them into a new token\n4. Repeat until desired vocabulary size is reached\n\n**Example:**\n- Text: `\"aaabdaaabac\"`\n- Most frequent pair: `\"aa\"` (appears 4 times)\n- After merge: `\"ZabdZabac\"` (where Z = \"aa\")\n- Continue merging...\n\nThis allows the tokenizer to learn common subwords like \"ing\", \"tion\", \"un\", etc."
  },
  {
   "cell_type": "markdown",
   "id": "wm2zwpk2te",
   "source": "## Step 0: Imports and Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "j13d0owl8vo",
   "source": "import rustbpe\nimport tiktoken\nimport pickle\n\n# Special tokens used in the tokenizer\nSPECIAL_TOKENS = [\n    \"<|bos|>\",  # Beginning of Sequence\n    \"<|user_start|>\", \"<|user_end|>\",\n    \"<|assistant_start|>\", \"<|assistant_end|>\",\n    \"<|python_start|>\", \"<|python_end|>\",\n    \"<|output_start|>\", \"<|output_end|>\",\n]\n\n# GPT-4 style regex pattern for pre-tokenization\nSPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n\nprint(f\"Number of special tokens: {len(SPECIAL_TOKENS)}\")\nprint(f\"Split pattern: {SPLIT_PATTERN[:50]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3yesn7myiix",
   "source": "## Step 1: Prepare Sample Training Data\n\nWe'll create a simple iterator of text that simulates what would be passed to the training function.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "28xtq7cer1rj",
   "source": "# Sample training data - in real use, this would be millions of documents\nsample_texts = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Hello, world! This is a test of tokenization.\",\n    \"Machine learning models need lots of training data.\",\n    \"The weather is beautiful today. The sun is shining.\",\n    \"Python programming is fun and powerful.\",\n    \"The cat sat on the mat. The dog ran in the park.\",\n] * 100  # Repeat to give more training data\n\n# Create an iterator (this is what train_from_iterator expects)\ntext_iterator = iter(sample_texts)\n\n# Let's also set a small vocabulary size for this demo\nVOCAB_SIZE = 512  # Much smaller than real models (GPT uses 50k-100k)\n\nprint(f\"Training data: {len(sample_texts)} documents\")\nprint(f\"Target vocabulary size: {VOCAB_SIZE}\")\nprint(f\"First document: {sample_texts[0]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8swbl33jet7",
   "source": "## Step 2: The Complete train_from_iterator Function\n\nHere's the full function we're going to dissect:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9k41xdh28h5",
   "source": "def train_from_iterator_complete(text_iterator, vocab_size):\n    \"\"\"\n    Complete train_from_iterator function from RustBPETokenizer\n    \"\"\"\n    # 1) train using rustbpe, Rust based tokenizer faster than Python\n    tokenizer = rustbpe.Tokenizer()\n\n    # the special tokens are inserted later, we don't train them here\n    vocab_size_no_special = vocab_size - len(SPECIAL_TOKENS)\n\n    assert vocab_size_no_special >= 256, f\"vocab_size_no_special must be at least 256, got {vocab_size_no_special}\"\n    \n    # Train the tokenizer\n    tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)\n    \n    # 2) construct the associated tiktoken encoding for inference\n    pattern = tokenizer.get_pattern()\n    mergeable_ranks_list = tokenizer.get_mergeable_ranks()\n    mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}\n    tokens_offset = len(mergeable_ranks)\n    special_tokens = {name: tokens_offset + i for i, name in enumerate(SPECIAL_TOKENS)}\n    \n    enc = tiktoken.Encoding(\n        name=\"rustbpe\",\n        pat_str=pattern,\n        mergeable_ranks=mergeable_ranks, # dict[bytes, int] (token bytes -> merge priority rank)\n        special_tokens=special_tokens, # dict[str, int] (special token name -> token id)\n    )\n    \n    return enc\n\nprint(\"Function defined. Let's break it down step by step next!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9zjje0ukm5a",
   "source": "---\n\n## STEP-BY-STEP BREAKDOWN\n\nNow let's execute each part of the function separately to understand what's happening.\n\n### STEP 2.1: Initialize the RustBPE Tokenizer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ayy9j5yj37",
   "source": "# STEP 2.1: Create a rustbpe.Tokenizer instance\ntokenizer = rustbpe.Tokenizer()\n\nprint(\"âœ“ Created rustbpe.Tokenizer instance\")\nprint(f\"  Type: {type(tokenizer)}\")\nprint(f\"  This is an empty tokenizer that will learn BPE merges from data\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wyoy485h5j",
   "source": "### STEP 2.2: Calculate Vocabulary Size (Reserve Space for Special Tokens)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jhbw8lkjj2",
   "source": "# STEP 2.2: Reserve space for special tokens\nvocab_size_no_special = VOCAB_SIZE - len(SPECIAL_TOKENS)\n\nprint(f\"âœ“ Calculated vocabulary sizes:\")\nprint(f\"  Total vocab size: {VOCAB_SIZE}\")\nprint(f\"  Special tokens: {len(SPECIAL_TOKENS)}\")\nprint(f\"  Vocab for BPE training: {vocab_size_no_special}\")\nprint()\nprint(f\"  Why? Special tokens are added AFTER training, so we train with fewer slots\")\nprint(f\"  The BPE algorithm will learn {vocab_size_no_special} tokens from the data\")\n\n# Sanity check\nassert vocab_size_no_special >= 256, f\"Need at least 256 for base bytes!\"\nprint(f\"\\n  âœ“ Passed sanity check (>= 256 for all byte values)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c9sl448dm2v",
   "source": "### STEP 2.3: Train the Tokenizer (THE MAIN EVENT!)\n\nThis is where the magic happens! The tokenizer will:\n1. Split text according to the SPLIT_PATTERN regex\n2. Start with 256 base byte tokens\n3. Iteratively find the most frequent pair of tokens and merge them\n4. Continue until we have `vocab_size_no_special` tokens",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "uv8unep1d6o",
   "source": "# STEP 2.3: Train the tokenizer!\n# Note: We need to recreate the iterator since we consumed it earlier\ntext_iterator = iter(sample_texts)\n\nprint(\"ðŸš€ Starting BPE training...\")\nprint(f\"   Training on {len(sample_texts)} documents\")\nprint(f\"   Target: {vocab_size_no_special} tokens (256 base + {vocab_size_no_special - 256} merges)\")\nprint()\n\n# This is the heavy lifting - learning which byte pairs to merge\ntokenizer.train_from_iterator(\n    text_iterator, \n    vocab_size_no_special, \n    pattern=SPLIT_PATTERN\n)\n\nprint(\"\\nâœ“ Training complete!\")\nprint(\"  The tokenizer has learned which byte pairs appear frequently\")\nprint(\"  and should be merged into single tokens\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0sd92vozrg1j",
   "source": "### STEP 2.4: Extract the Pattern\n\nRetrieve the regex pattern used for pre-tokenization (should be the same one we passed in).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9ob9oihaswb",
   "source": "# STEP 2.4: Get the pattern back from the trained tokenizer\npattern = tokenizer.get_pattern()\n\nprint(\"âœ“ Retrieved pattern from trained tokenizer\")\nprint(f\"  Pattern: {pattern[:80]}...\")\nprint()\nprint(\"  This is the same regex we passed in - it's needed for tiktoken later\")\nprint(\"  so that inference uses the same pre-tokenization as training\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lx0sm8fj5j",
   "source": "### STEP 2.5: Extract Mergeable Ranks (The Learned BPE Merges!)\n\nThis is the core output of training - the learned merge rules.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ev2x445epx4",
   "source": "# STEP 2.5: Get the mergeable ranks (the learned BPE vocabulary)\nmergeable_ranks_list = tokenizer.get_mergeable_ranks()\n\nprint(\"âœ“ Retrieved mergeable ranks from trained tokenizer\")\nprint(f\"  Type: {type(mergeable_ranks_list)}\")\nprint(f\"  Length: {len(mergeable_ranks_list)} tokens\")\nprint()\nprint(\"  What is this? A list of (token_bytes, rank) pairs\")\nprint(\"  - token_bytes: The bytes that make up this token\")\nprint(\"  - rank: The priority/order in which this merge was learned (0 = first)\")\nprint()\nprint(\"  First 10 tokens (these are the base bytes 0-9):\")\nfor i in range(10):\n    token_bytes, rank = mergeable_ranks_list[i]\n    print(f\"    {i}: bytes={list(token_bytes)}, rank={rank}, char='{chr(token_bytes[0]) if len(token_bytes)==1 and 32<=token_bytes[0]<127 else '?'}'\")\nprint()\nprint(\"  Last 10 tokens (these are the most recently learned merges):\")\nfor i in range(-10, 0):\n    token_bytes, rank = mergeable_ranks_list[i]\n    try:\n        text = token_bytes.decode('utf-8')\n    except:\n        text = repr(token_bytes)\n    print(f\"    Token {len(mergeable_ranks_list)+i}: rank={rank}, bytes={text}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wrww4jgflx",
   "source": "### STEP 2.6: Convert to Dictionary Format for tiktoken\n\ntiktoken expects `mergeable_ranks` as a dict mapping `bytes -> int` (rank).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6exn838iwhr",
   "source": "# STEP 2.6: Convert list to dict for tiktoken\n# tiktoken wants: dict[bytes, int] where int is the merge rank\nmergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}\n\nprint(\"âœ“ Converted to dictionary format\")\nprint(f\"  Type: {type(mergeable_ranks)}\")\nprint(f\"  Length: {len(mergeable_ranks)}\")\nprint()\nprint(\"  Example entries:\")\nfor i, (token_bytes, rank) in enumerate(list(mergeable_ranks.items())[:5]):\n    try:\n        text = token_bytes.decode('utf-8')\n    except:\n        text = repr(token_bytes)\n    print(f\"    {text!r} -> rank {rank}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0fmzujfih69i",
   "source": "### STEP 2.7: Add Special Tokens\n\nSpecial tokens get IDs starting AFTER all the learned BPE tokens.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jdz4qv8i299",
   "source": "# STEP 2.7: Create special tokens mapping\n# Special tokens get IDs starting from tokens_offset (after all BPE tokens)\ntokens_offset = len(mergeable_ranks)\nspecial_tokens = {name: tokens_offset + i for i, name in enumerate(SPECIAL_TOKENS)}\n\nprint(\"âœ“ Created special tokens mapping\")\nprint(f\"  Offset (first special token ID): {tokens_offset}\")\nprint(f\"  Number of special tokens: {len(special_tokens)}\")\nprint()\nprint(\"  Special token mappings:\")\nfor name, token_id in special_tokens.items():\n    print(f\"    '{name}' -> ID {token_id}\")\nprint()\nprint(f\"  Total vocabulary size: {tokens_offset + len(special_tokens)}\")\nprint(f\"  (Should equal our target: {VOCAB_SIZE})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "unw44jun3x",
   "source": "### STEP 2.8: Create the tiktoken Encoding Object (Final Output!)\n\nThis is what gets returned and used for fast inference.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "yti6ujv6dsh",
   "source": "# STEP 2.8: Create the tiktoken.Encoding object\nenc = tiktoken.Encoding(\n    name=\"rustbpe\",\n    pat_str=pattern,              # The regex pattern for pre-tokenization\n    mergeable_ranks=mergeable_ranks,  # The learned BPE merges\n    special_tokens=special_tokens,    # Our special tokens\n)\n\nprint(\"âœ“ Created tiktoken.Encoding object!\")\nprint(f\"  Type: {type(enc)}\")\nprint(f\"  Name: {enc.name}\")\nprint(f\"  Vocabulary size: {enc.n_vocab}\")\nprint()\nprint(\"  This encoding object can now be used for FAST tokenization:\")\nprint(\"  - encode(): text -> token IDs\")\nprint(\"  - decode(): token IDs -> text\")\nprint()\nprint(\"ðŸŽ‰ Training complete! The tokenizer is ready to use.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rpamu576iyd",
   "source": "---\n\n## Step 3: Test the Trained Tokenizer!\n\nLet's see the tokenizer in action.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vm83jssejc",
   "source": "# Test encoding\ntest_text = \"Hello, world! The quick brown fox jumps.\"\n\n# Encode\ntoken_ids = enc.encode_ordinary(test_text)\nprint(f\"Original text: {test_text}\")\nprint(f\"Token IDs: {token_ids}\")\nprint(f\"Number of tokens: {len(token_ids)}\")\nprint()\n\n# Decode\ndecoded_text = enc.decode(token_ids)\nprint(f\"Decoded text: {decoded_text}\")\nprint(f\"Match original? {decoded_text == test_text}\")\nprint()\n\n# Show individual tokens\nprint(\"Individual tokens:\")\nfor i, token_id in enumerate(token_ids):\n    token_text = enc.decode([token_id])\n    print(f\"  {i}: ID={token_id:4d} -> {token_text!r}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "izy2y7ihbop",
   "source": "### Test Special Tokens",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5xn8sukxvhk",
   "source": "# Test special token encoding\nbos_id = enc.encode_single_token(\"<|bos|>\")\nuser_start_id = enc.encode_single_token(\"<|user_start|>\")\n\nprint(\"Special token IDs:\")\nprint(f\"  <|bos|> -> {bos_id}\")\nprint(f\"  <|user_start|> -> {user_start_id}\")\nprint()\n\n# Test with a simple conversation-like sequence\nconversation_text = \"<|bos|><|user_start|>Hello!<|user_end|><|assistant_start|>Hi there!<|assistant_end|>\"\n# Note: encode_ordinary won't process special tokens in the text\n# In real usage, RustBPETokenizer builds these sequences programmatically\n\nprint(\"In real usage, special tokens are added programmatically,\")\nprint(\"not as text to be encoded.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4s6rlsgh0tj",
   "source": "---\n\n## Summary: What train_from_iterator Does\n\n### Input:\n- `text_iterator`: Iterator over text documents\n- `vocab_size`: Desired vocabulary size (e.g., 512, 50000, 100000)\n\n### Process:\n1. **Initialize** rustbpe tokenizer\n2. **Calculate** vocab size without special tokens (need room for them)\n3. **Train** BPE algorithm:\n   - Pre-tokenize text using SPLIT_PATTERN regex\n   - Start with 256 base byte tokens\n   - Iteratively merge most frequent byte pairs\n   - Continue until reaching target vocabulary size\n4. **Extract** learned merge rules (mergeable_ranks)\n5. **Convert** to tiktoken-compatible format\n6. **Add** special tokens at the end\n7. **Create** tiktoken.Encoding object for fast inference\n\n### Output:\n- A `tiktoken.Encoding` object that can:\n  - `encode()`: Convert text to token IDs\n  - `decode()`: Convert token IDs to text\n  - Handle special tokens\n  - Run efficiently in production\n\n### Key Insight:\n**Train with Rust (fast), Infer with tiktoken (also fast)** - Best of both worlds!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "47kel5hmvx5",
   "source": "---\n\n## Bonus: What Did BPE Learn?\n\nLet's examine some of the merged tokens to see what patterns BPE discovered.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "rwfoawp3h1m",
   "source": "# Look at tokens beyond the base 256 bytes (these are the learned merges)\nprint(\"Learned multi-byte tokens (showing a sample):\")\nprint()\n\nlearned_tokens = []\nfor token_bytes, rank in mergeable_ranks.items():\n    if len(token_bytes) > 1:  # Multi-byte tokens (learned merges)\n        try:\n            text = token_bytes.decode('utf-8')\n            learned_tokens.append((rank, text, token_bytes))\n        except:\n            pass  # Skip non-UTF8 tokens\n\n# Sort by rank (earlier ranks = more frequent merges)\nlearned_tokens.sort()\n\nprint(f\"Total learned multi-byte tokens: {len(learned_tokens)}\")\nprint()\nprint(\"First 30 learned merges (most frequent patterns):\")\nfor i, (rank, text, token_bytes) in enumerate(learned_tokens[:30]):\n    print(f\"  Rank {rank:3d}: {text!r:20s} (len={len(token_bytes)})\")\n\nprint()\nprint(\"These are common patterns BPE discovered in our training data!\")\nprint(\"For example, you might see tokens like:\")\nprint(\"  - ' the' (space + the)\")\nprint(\"  - 'ing' (common suffix)\")\nprint(\"  - 'er', 'ed' (common endings)\")\nprint(\"  - Common words that appear frequently\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hjcgk94bs4n",
   "source": "### Why is BPE Better Than Character-Level?\n\nLet's compare BPE tokenization to simple character-level tokenization.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8zm0t06vprn",
   "source": "comparison_text = \"The quick brown fox jumps over the lazy dog. Machine learning is amazing!\"\n\n# BPE tokenization\nbpe_tokens = enc.encode_ordinary(comparison_text)\nbpe_count = len(bpe_tokens)\n\n# Character-level (naive approach)\nchar_count = len(comparison_text)\n\nprint(f\"Text: {comparison_text}\")\nprint()\nprint(f\"Character-level: {char_count} tokens\")\nprint(f\"BPE tokenization: {bpe_count} tokens\")\nprint(f\"Compression ratio: {char_count / bpe_count:.2f}x\")\nprint()\nprint(\"Benefits of BPE:\")\nprint(\"  1. Fewer tokens = shorter sequences for the model to process\")\nprint(\"  2. Common words/subwords are single tokens (more efficient)\")\nprint(\"  3. Rare words split into known subwords (better generalization)\")\nprint(\"  4. Variable-length encoding (optimal for language structure)\")\nprint()\nprint(\"BPE tokens:\")\nfor i, tid in enumerate(bpe_tokens[:20]):  # Show first 20\n    print(f\"  {enc.decode([tid])!r}\", end=\" \")\n    if i > 0 and (i+1) % 10 == 0:\n        print()\nprint(\"\\n...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xpcjpnhdivl",
   "source": "---\n\n## Visual Flow Diagram\n\n```\ntrain_from_iterator(text_iterator, vocab_size)\nâ”‚\nâ”œâ”€ Step 1: Initialize rustbpe.Tokenizer()\nâ”‚          â””â”€> Empty tokenizer ready to learn\nâ”‚\nâ”œâ”€ Step 2: Calculate vocab_size_no_special\nâ”‚          â””â”€> vocab_size - 9 special tokens\nâ”‚\nâ”œâ”€ Step 3: Train BPE Algorithm\nâ”‚          â”œâ”€> Input: text documents\nâ”‚          â”œâ”€> Pre-tokenize with SPLIT_PATTERN regex\nâ”‚          â”œâ”€> Start with 256 base bytes\nâ”‚          â”œâ”€> Merge frequent pairs iteratively\nâ”‚          â””â”€> Output: learned merge rules\nâ”‚\nâ”œâ”€ Step 4: Extract Training Results\nâ”‚          â”œâ”€> pattern = tokenizer.get_pattern()\nâ”‚          â””â”€> mergeable_ranks = tokenizer.get_mergeable_ranks()\nâ”‚                 â”‚\nâ”‚                 â””â”€> List[(bytes, rank)] of all tokens\nâ”‚\nâ”œâ”€ Step 5: Convert to tiktoken Format\nâ”‚          â””â”€> Dict[bytes -> int]\nâ”‚\nâ”œâ”€ Step 6: Add Special Tokens\nâ”‚          â””â”€> Map special token names to IDs\nâ”‚             (IDs start after all BPE tokens)\nâ”‚\nâ””â”€ Step 7: Create tiktoken.Encoding\n           â””â”€> Fast inference-ready tokenizer\n               â”œâ”€> encode(text) -> [token_ids]\n               â””â”€> decode([token_ids]) -> text\n```\n\n---\n\n## Next Steps\n\nTo use this in RustBPETokenizer class:\n1. Wrap the tiktoken.Encoding in the class\n2. Add helper methods (get_bos_token_id, render_conversation, etc.)\n3. Implement save/load functionality\n\nSee `nanochat/tokenizer.py` for the full implementation!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}