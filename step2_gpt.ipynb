{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# Understanding GPT Architecture - Deep Dive\n\nThis notebook breaks down the GPT (Generative Pre-trained Transformer) architecture step by step.\n\n## Key Features of This Implementation\n\nThis is a **modern, optimized** GPT implementation with several improvements over the original:\n\n1. **Rotary Embeddings (RoPE)** - Better positional encoding than learned embeddings\n2. **QK Normalization** - Stabilizes training\n3. **Untied Weights** - Separate weights for token embedding and output layer\n4. **ReLU²** - Squared ReLU activation in MLP (smoother than GELU)\n5. **RMSNorm** - Simpler, faster normalization (no learnable params)\n6. **No Bias** - Cleaner, fewer parameters\n7. **Group-Query Attention (GQA)** - Efficient inference with KV cache\n\n## What We'll Cover\n\n1. **Configuration** - Model hyperparameters\n2. **Normalization** - RMSNorm explained\n3. **Rotary Embeddings** - How positional information is encoded\n4. **Attention Mechanism** - Multi-head self-attention with GQA\n5. **MLP (Feedforward)** - The \"thinking\" layer\n6. **Transformer Block** - Attention + MLP combined\n7. **Full GPT Model** - Putting it all together\n8. **Forward Pass** - How data flows through the model"
  },
  {
   "cell_type": "markdown",
   "id": "soi4mqa9no",
   "source": "## Section 1: Configuration and Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "719n639paf5",
   "source": "import math\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# GPT Configuration\n@dataclass\nclass GPTConfig:\n    sequence_len: int = 1024  # Maximum sequence length (context window)\n    vocab_size: int = 50304   # Vocabulary size (number of unique tokens)\n    n_layer: int = 12         # Number of transformer blocks\n    n_head: int = 6           # Number of query attention heads\n    n_kv_head: int = 6        # Number of key/value heads (for GQA)\n    n_embd: int = 768         # Embedding dimension (model width)\n\n# Create a small config for demonstration\nconfig = GPTConfig(\n    sequence_len=128,\n    vocab_size=512,\n    n_layer=4,\n    n_head=4,\n    n_kv_head=4,  # Same as n_head = standard multi-head attention\n    n_embd=256\n)\n\nprint(\"GPT Configuration:\")\nprint(f\"  Sequence length: {config.sequence_len} tokens\")\nprint(f\"  Vocabulary size: {config.vocab_size}\")\nprint(f\"  Number of layers: {config.n_layer}\")\nprint(f\"  Number of heads: {config.n_head}\")\nprint(f\"  Embedding dimension: {config.n_embd}\")\nprint(f\"  Head dimension: {config.n_embd // config.n_head}\")\nprint()\nprint(f\"Why these values?\")\nprint(f\"  - sequence_len: How much context the model can see\")\nprint(f\"  - vocab_size: How many unique tokens (from tokenizer)\")\nprint(f\"  - n_layer: Depth of the network (more = more capacity)\")\nprint(f\"  - n_head: Parallel attention computations (more = richer representations)\")\nprint(f\"  - n_embd: Width of the network (larger = more parameters)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "i9gbhd5956",
   "source": "## Section 2: RMSNorm - Root Mean Square Normalization\n\n**Why normalize?** Neural networks train better when activations are kept in a reasonable range. Normalization prevents exploding/vanishing gradients.\n\n**Why RMSNorm over LayerNorm?**\n- Simpler: No learnable scale/shift parameters\n- Faster: Fewer computations\n- Works just as well in practice\n\n**Formula:** `RMSNorm(x) = x / RMS(x)` where `RMS(x) = sqrt(mean(x²))`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3a95mwv0w7i",
   "source": "def norm(x):\n    \"\"\"\n    Purely functional RMSNorm with no learnable parameters\n    \"\"\"\n    return F.rms_norm(x, (x.size(-1),))\n\n# Test it with a sample tensor\nx = torch.randn(2, 4, 256)  # (batch, seq_len, embd_dim)\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Input mean: {x.mean():.4f}, std: {x.std():.4f}\")\nprint()\n\nx_normed = norm(x)\nprint(f\"After RMSNorm:\")\nprint(f\"  Output shape: {x_normed.shape}\")\nprint(f\"  Output mean: {x_normed.mean():.4f}, std: {x_normed.std():.4f}\")\nprint()\n\n# Verify RMSNorm formula manually\nrms = torch.sqrt((x ** 2).mean(dim=-1, keepdim=True))\nx_manual = x / rms\nprint(f\"Manual RMSNorm matches? {torch.allclose(x_normed, x_manual, atol=1e-6)}\")\nprint()\nprint(\"Key insight: RMSNorm scales each vector to have RMS = 1\")\nprint(\"This keeps activations in a stable range throughout the network\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "x1jysfd0ni",
   "source": "## Section 3: Rotary Positional Embeddings (RoPE)\n\n**The Problem:** Transformers have no inherent notion of position. \"cat sat mat\" and \"mat sat cat\" look the same!\n\n**Old Solution:** Add positional embeddings to token embeddings (GPT-2 style)\n\n**Better Solution:** Rotary Embeddings (RoPE)\n- Apply rotation to query and key vectors based on their position\n- Encodes **relative** position (distance between tokens) rather than absolute\n- Better generalization to longer sequences\n- No extra parameters to learn!\n\n### How RoPE Works\n\n1. **Frequency Computation:** Different dimensions get different rotation frequencies\n2. **Rotation:** Rotate pairs of dimensions by an angle proportional to position\n3. **Application:** Apply to both queries and keys in attention\n\n**Key Insight:** The dot product between rotated Q and K encodes their relative distance!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "oly0f108st",
   "source": "# STEP 1: Precompute Rotary Embeddings\n\ndef precompute_rotary_embeddings(seq_len, head_dim, base=10000):\n    \"\"\"\n    Precompute cos and sin values for rotary embeddings\n    \n    Args:\n        seq_len: Maximum sequence length\n        head_dim: Dimension of each attention head\n        base: Base for frequency computation (10000 is standard)\n    \n    Returns:\n        cos, sin: Precomputed rotation matrices\n    \"\"\"\n    # Step 1: Compute inverse frequencies for each dimension pair\n    # We process dimensions in pairs: (0,1), (2,3), (4,5), ...\n    channel_range = torch.arange(0, head_dim, 2, dtype=torch.float32)\n    inv_freq = 1.0 / (base ** (channel_range / head_dim))\n    \n    print(f\"Step 1: Compute frequencies\")\n    print(f\"  Head dimension: {head_dim}\")\n    print(f\"  Number of dimension pairs: {head_dim // 2}\")\n    print(f\"  Inverse frequencies shape: {inv_freq.shape}\")\n    print(f\"  inv_freq values (first 4): {inv_freq[:4].tolist()}\")\n    print()\n    \n    # Step 2: Create position indices\n    t = torch.arange(seq_len, dtype=torch.float32)\n    \n    # Step 3: Compute rotation angles = position * frequency\n    # outer product: each position gets paired with each frequency\n    freqs = torch.outer(t, inv_freq)  # (seq_len, head_dim/2)\n    \n    print(f\"Step 2: Compute rotation angles\")\n    print(f\"  Position range: 0 to {seq_len-1}\")\n    print(f\"  Frequencies shape: {freqs.shape}\")\n    print(f\"  Frequencies at position 0: {freqs[0, :4].tolist()}\")\n    print(f\"  Frequencies at position 10: {freqs[10, :4].tolist()}\")\n    print()\n    \n    # Step 4: Compute cos and sin\n    cos, sin = freqs.cos(), freqs.sin()\n    \n    # Step 5: Reshape for broadcasting in attention\n    # (1, seq_len, 1, head_dim/2) for broadcasting over (B, H, T, D)\n    cos = cos[None, :, None, :]\n    sin = sin[None, :, None, :]\n    \n    print(f\"Step 3: Final cos/sin tensors\")\n    print(f\"  cos shape: {cos.shape} (batch, seq_len, head, head_dim/2)\")\n    print(f\"  sin shape: {sin.shape}\")\n    print()\n    \n    return cos, sin\n\n# Demo with our config\nhead_dim = config.n_embd // config.n_head\ncos, sin = precompute_rotary_embeddings(config.sequence_len, head_dim)\n\nprint(f\"✓ Rotary embeddings precomputed for {config.sequence_len} positions\")\nprint(f\"  These are computed once and cached, not learned!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4k4f23nos6",
   "source": "# STEP 2: Apply Rotary Embeddings\n\ndef apply_rotary_emb(x, cos, sin):\n    \"\"\"\n    Apply rotary embeddings to query or key tensor\n    \n    The rotation is applied to pairs of dimensions:\n    - Dimensions (0,1) rotate together\n    - Dimensions (2,3) rotate together\n    - etc.\n    \n    Mathematically, for each pair (x1, x2):\n        y1 = x1 * cos + x2 * sin\n        y2 = -x1 * sin + x2 * cos\n    \n    This is a 2D rotation matrix!\n    \"\"\"\n    assert x.ndim == 4  # (batch, n_head, seq_len, head_dim)\n    d = x.shape[3] // 2\n    \n    # Split into two halves (pairs of dimensions)\n    x1, x2 = x[..., :d], x[..., d:]\n    \n    # Rotate each pair\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    \n    # Concatenate back\n    return torch.cat([y1, y2], dim=3)\n\n# Demonstrate on sample query vectors\nB, H, T, D = 2, 4, 8, 64  # batch, heads, seq_len, head_dim\nq = torch.randn(B, H, T, D)\n\nprint(f\"Query tensor shape: {q.shape}\")\nprint(f\"  B={B} (batch), H={H} (heads), T={T} (seq_len), D={D} (head_dim)\")\nprint()\n\n# Apply rotary embeddings\nq_rotated = apply_rotary_emb(q, cos[:, :T], sin[:, :T])\n\nprint(f\"After rotation: {q_rotated.shape}\")\nprint(f\"  Same shape, but now encodes positional information!\")\nprint()\n\n# Key insight: The dot product between rotated Q and K at positions i and j\n# depends on (i - j), giving us relative positional encoding!\nprint(\"Why this works:\")\nprint(\"  - Queries and keys at position i get rotated by angle i*freq\")\nprint(\"  - The dot product Q[i] · K[j] includes cos((i-j)*freq) terms\")\nprint(\"  - This encodes the RELATIVE distance (i-j) between positions\")\nprint(\"  - Model learns: 'pay attention to tokens N positions away'\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xxz71nty8r",
   "source": "## Section 4: Causal Self-Attention Mechanism\n\n**Attention is the core innovation of Transformers.** It allows the model to \"look at\" and weigh the importance of all previous tokens when processing each token.\n\n### The Attention Formula\n\n```\nAttention(Q, K, V) = softmax(Q @ K^T / √d) @ V\n```\n\nWhere:\n- **Q (Query)**: \"What am I looking for?\"\n- **K (Key)**: \"What do I contain?\"\n- **V (Value)**: \"What information do I carry?\"\n\n### Multi-Head Attention\n\nInstead of one attention, we use multiple \"heads\" in parallel:\n- Each head learns different patterns\n- Head 1 might focus on syntax, Head 2 on semantics, etc.\n- Outputs are concatenated and projected back\n\n### Group-Query Attention (GQA)\n\n**Optimization:** Share K and V across multiple Q heads\n- Standard: `n_head` sets of Q, K, V\n- GQA: `n_head` Q heads, but only `n_kv_head` K/V heads\n- Saves memory and computation during inference\n- Example: 8 Q heads might share 2 K/V heads (4x reduction!)\n\n### Causal Masking\n\n**Key constraint:** Token at position `i` can only attend to positions `≤ i`\n- Prevents \"looking into the future\"\n- Essential for autoregressive generation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "svkkcxonqg",
   "source": "class CausalSelfAttention(nn.Module):\n    \"\"\"\n    Multi-head causal self-attention with Group-Query Attention (GQA) support\n    \"\"\"\n    def __init__(self, config, layer_idx):\n        super().__init__()\n        self.layer_idx = layer_idx\n        self.n_head = config.n_head          # Number of query heads\n        self.n_kv_head = config.n_kv_head    # Number of key/value heads\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        \n        # Ensure dimensions are compatible\n        assert self.n_embd % self.n_head == 0, \"n_embd must be divisible by n_head\"\n        assert self.n_kv_head <= self.n_head, \"Can't have more KV heads than Q heads\"\n        assert self.n_head % self.n_kv_head == 0, \"n_head must be divisible by n_kv_head\"\n        \n        # Projection matrices (no bias!)\n        self.c_q = nn.Linear(self.n_embd, self.n_head * self.head_dim, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n    \n    def forward(self, x, cos_sin, kv_cache=None):\n        B, T, C = x.size()  # batch, sequence length, embedding dim\n        \n        # STEP 1: Project input to Q, K, V\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_kv_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_kv_head, self.head_dim)\n        \n        # STEP 2: Apply rotary embeddings to Q and K\n        cos, sin = cos_sin\n        q = apply_rotary_emb(q, cos, sin)\n        k = apply_rotary_emb(k, cos, sin)\n        \n        # STEP 3: QK Normalization (stabilizes training)\n        q, k = norm(q), norm(k)\n        \n        # STEP 4: Transpose for attention computation\n        # (B, T, H, D) -> (B, H, T, D) - make head dimension the batch dimension\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n        \n        # STEP 5: Compute attention\n        # For simplicity, we'll just use scaled_dot_product_attention\n        enable_gqa = self.n_head != self.n_kv_head\n        y = F.scaled_dot_product_attention(q, k, v, is_causal=True, enable_gqa=enable_gqa)\n        \n        # STEP 6: Reshape and project output\n        y = y.transpose(1, 2).contiguous().view(B, T, -1)\n        y = self.c_proj(y)\n        \n        return y\n\n# Create and test attention module\nattn = CausalSelfAttention(config, layer_idx=0)\n\nprint(\"✓ Causal Self-Attention Module Created\")\nprint(f\"  Query heads: {attn.n_head}\")\nprint(f\"  KV heads: {attn.n_kv_head}\")\nprint(f\"  Head dimension: {attn.head_dim}\")\nprint(f\"  GQA enabled: {attn.n_head != attn.n_kv_head}\")\nprint()\nprint(\"Parameters:\")\nprint(f\"  c_q: {attn.c_q.weight.shape} ({attn.c_q.weight.numel():,} params)\")\nprint(f\"  c_k: {attn.c_k.weight.shape} ({attn.c_k.weight.numel():,} params)\")\nprint(f\"  c_v: {attn.c_v.weight.shape} ({attn.c_v.weight.numel():,} params)\")\nprint(f\"  c_proj: {attn.c_proj.weight.shape} ({attn.c_proj.weight.numel():,} params)\")\n\n# Test forward pass\nx_test = torch.randn(2, 8, config.n_embd)\ny_test = attn(x_test, (cos[:, :8], sin[:, :8]))\nprint()\nprint(f\"✓ Forward pass successful!\")\nprint(f\"  Input: {x_test.shape}\")\nprint(f\"  Output: {y_test.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wn7mdk0xzqo",
   "source": "### Attention Step-by-Step Visualization\n\nLet's manually compute attention for a tiny example to see what's happening:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kdagokom81",
   "source": "# Manual attention computation for educational purposes\n# Simplified: 1 head, 4 tokens, dimension 8\n\nseq_len = 4\ndim = 8\n\n# Random Q, K, V (normally these come from linear projections)\nQ = torch.randn(1, 1, seq_len, dim)  # (batch, heads, seq_len, dim)\nK = torch.randn(1, 1, seq_len, dim)\nV = torch.randn(1, 1, seq_len, dim)\n\nprint(\"=\"*60)\nprint(\"MANUAL ATTENTION COMPUTATION\")\nprint(\"=\"*60)\nprint()\n\n# Step 1: Compute attention scores (Q @ K^T)\nscores = Q @ K.transpose(-2, -1)  # (1, 1, 4, 4)\nprint(\"Step 1: Compute Q @ K^T\")\nprint(f\"  Scores shape: {scores.shape}\")\nprint(f\"  Scores (before scaling):\")\nprint(scores[0, 0].numpy())\nprint()\n\n# Step 2: Scale by sqrt(dim)\nscores = scores / math.sqrt(dim)\nprint(f\"Step 2: Scale by 1/√{dim} = {1/math.sqrt(dim):.3f}\")\nprint(f\"  Scaled scores:\")\nprint(scores[0, 0].numpy())\nprint()\n\n# Step 3: Apply causal mask (prevent looking ahead)\nmask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\nscores = scores.masked_fill(mask, float('-inf'))\nprint(\"Step 3: Apply causal mask (set future positions to -inf)\")\nprint(\"  Mask (True = masked):\")\nprint(mask.int().numpy())\nprint(f\"  Masked scores:\")\nprint(scores[0, 0].numpy())\nprint()\n\n# Step 4: Softmax to get attention weights\nattn_weights = F.softmax(scores, dim=-1)\nprint(\"Step 4: Softmax (convert to probabilities)\")\nprint(\"  Attention weights (each row sums to 1):\")\nprint(attn_weights[0, 0].numpy())\nprint()\nprint(\"  Interpretation:\")\nprint(\"    Row 0: Token 0 attends 100% to itself (can't see future)\")\nprint(\"    Row 1: Token 1 attends to tokens 0 and 1\")\nprint(\"    Row 2: Token 2 attends to tokens 0, 1, and 2\")\nprint(\"    Row 3: Token 3 attends to all tokens 0-3\")\nprint()\n\n# Step 5: Weighted sum of values\noutput = attn_weights @ V\nprint(\"Step 5: Multiply attention weights by V (weighted sum)\")\nprint(f\"  Output shape: {output.shape}\")\nprint()\n\nprint(\"=\"*60)\nprint(\"Key Takeaway:\")\nprint(\"  Each token's output is a weighted combination of all\")\nprint(\"  previous tokens' values, where weights come from Q·K\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "jrko1ieed2p",
   "source": "## Section 5: MLP (Multi-Layer Perceptron) - The Feedforward Network\n\nAfter attention gathers information from other tokens, the **MLP processes this information**.\n\n### Standard MLP Structure\n\n```\nx → Linear(expand 4x) → Activation → Linear(project back) → output\n```\n\n### Why 4x Expansion?\n\n- Attention: `n_embd → n_embd` (no change in dimension)\n- MLP: `n_embd → 4*n_embd → n_embd`\n- The expansion gives the model \"room to think\"\n- 4x is empirically found to work well (from original Transformer paper)\n\n### ReLU² Activation\n\nThis implementation uses **ReLU²** instead of GELU:\n- `ReLU²(x) = ReLU(x)² = max(0, x)²`\n- Smoother than standard ReLU\n- Faster to compute than GELU\n- Works well in practice\n\n### Role of MLP\n\n- **Attention**: Gathers information (\"what's relevant?\")\n- **MLP**: Processes information (\"what does it mean?\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "46dpki4b99a",
   "source": "class MLP(nn.Module):\n    \"\"\"\n    Simple MLP with 4x expansion and ReLU² activation\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        # Expand by 4x\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        # Project back to original size\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n    \n    def forward(self, x):\n        # Expand\n        x = self.c_fc(x)\n        # Activate with ReLU²\n        x = F.relu(x).square()\n        # Project back\n        x = self.c_proj(x)\n        return x\n\n# Create and test MLP\nmlp = MLP(config)\n\nprint(\"✓ MLP Module Created\")\nprint()\nprint(\"Architecture:\")\nprint(f\"  Input:  {config.n_embd} dimensions\")\nprint(f\"  Expand: {4 * config.n_embd} dimensions (4x)\")\nprint(f\"  Output: {config.n_embd} dimensions\")\nprint()\nprint(\"Parameters:\")\nprint(f\"  c_fc:   {mlp.c_fc.weight.shape} ({mlp.c_fc.weight.numel():,} params)\")\nprint(f\"  c_proj: {mlp.c_proj.weight.shape} ({mlp.c_proj.weight.numel():,} params)\")\nprint(f\"  Total:  {mlp.c_fc.weight.numel() + mlp.c_proj.weight.numel():,} params\")\nprint()\n\n# Test forward pass\nx_test = torch.randn(2, 8, config.n_embd)\ny_test = mlp(x_test)\n\nprint(f\"✓ Forward pass successful!\")\nprint(f\"  Input:  {x_test.shape}\")\nprint(f\"  Output: {y_test.shape}\")\nprint()\n\n# Visualize ReLU² activation\nx_act = torch.linspace(-2, 2, 100)\nrelu_act = F.relu(x_act)\nrelu2_act = F.relu(x_act).square()\n\nprint(\"Comparing activations at x=0, 0.5, 1, 1.5, 2:\")\nfor val in [0, 0.5, 1.0, 1.5, 2.0]:\n    x_val = torch.tensor([val])\n    relu = F.relu(x_val).item()\n    relu2 = F.relu(x_val).square().item()\n    print(f\"  x={val:.1f}: ReLU={relu:.3f}, ReLU²={relu2:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ixbw2mkra7",
   "source": "## Section 6: Transformer Block - Combining Attention and MLP\n\nA **Transformer Block** is the fundamental building block, combining attention and MLP with **residual connections** and **layer normalization**.\n\n### Structure (Pre-Norm Architecture)\n\n```\nx → norm → attention → (+) → norm → MLP → (+) → output\n    ↓__________________|      ↓______________|\n       residual connection    residual connection\n```\n\n### Key Design Choices\n\n1. **Pre-Norm** (vs Post-Norm):\n   - Normalize BEFORE attention/MLP, not after\n   - More stable training, especially for deep networks\n   - Modern standard (GPT-3, LLaMA, etc.)\n\n2. **Residual Connections** (`x = x + f(x)`):\n   - Allow gradients to flow directly through the network\n   - Prevent vanishing gradients in deep networks\n   - Model can learn identity function easily (just set f(x)=0)\n\n3. **Why This Works**:\n   - Each block makes a small \"update\" to the representation\n   - Information flows through both the residual path (untouched) and the transformation path\n   - Deep networks become easier to train",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6v63wf3mbv",
   "source": "class Block(nn.Module):\n    \"\"\"\n    Transformer block: attention + MLP with residual connections\n    \"\"\"\n    def __init__(self, config, layer_idx):\n        super().__init__()\n        self.attn = CausalSelfAttention(config, layer_idx)\n        self.mlp = MLP(config)\n    \n    def forward(self, x, cos_sin, kv_cache=None):\n        # Attention block with residual connection\n        # Pre-norm: normalize first, then apply attention, then add residual\n        x = x + self.attn(norm(x), cos_sin, kv_cache)\n        \n        # MLP block with residual connection\n        # Pre-norm: normalize first, then apply MLP, then add residual\n        x = x + self.mlp(norm(x))\n        \n        return x\n\n# Create and test a single transformer block\nblock = Block(config, layer_idx=0)\n\nprint(\"✓ Transformer Block Created\")\nprint()\nprint(\"Structure:\")\nprint(\"  1. x = x + attention(norm(x))  ← Attention with residual\")\nprint(\"  2. x = x + mlp(norm(x))        ← MLP with residual\")\nprint()\n\n# Count parameters\nn_params = sum(p.numel() for p in block.parameters())\nn_attn = sum(p.numel() for p in block.attn.parameters())\nn_mlp = sum(p.numel() for p in block.mlp.parameters())\n\nprint(\"Parameters:\")\nprint(f\"  Attention: {n_attn:,}\")\nprint(f\"  MLP:       {n_mlp:,}\")\nprint(f\"  Total:     {n_params:,}\")\nprint(f\"  MLP is {n_mlp/n_attn:.1f}x larger than attention!\")\nprint()\n\n# Test forward pass\nx_test = torch.randn(2, 8, config.n_embd)\ny_test = block(x_test, (cos[:, :8], sin[:, :8]))\n\nprint(f\"✓ Forward pass successful!\")\nprint(f\"  Input:  {x_test.shape}\")\nprint(f\"  Output: {y_test.shape}\")\nprint()\n\n# Demonstrate the residual connection\nx_small = torch.randn(1, 4, config.n_embd)\nx_input = x_small.clone()\n\n# If we zero out the block's parameters, we get identity function\nwith torch.no_grad():\n    for p in block.parameters():\n        p.zero_()\n\nx_output = block(x_small, (cos[:, :4], sin[:, :4]))\n\nprint(\"Residual Connection Demonstration:\")\nprint(\"  When all parameters are zero:\")\nprint(f\"  Input == Output? {torch.allclose(x_input, x_output)}\")\nprint(\"  This shows the residual connection allows identity mapping!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f4pvacu4lwd",
   "source": "## Section 7: Complete GPT Model - Putting It All Together\n\nThe full GPT model stacks multiple transformer blocks and adds input/output layers.\n\n### Architecture Overview\n\n```\nToken IDs (integers)\n       ↓\nToken Embedding (wte) → Vectors\n       ↓\n   RMSNorm (stabilize)\n       ↓\n Block 1 (attn + MLP)\n       ↓\n Block 2 (attn + MLP)\n       ↓\n      ...\n       ↓\n Block N (attn + MLP)\n       ↓\n   RMSNorm (final)\n       ↓\nLanguage Model Head (lm_head) → Logits\n       ↓\n   Softmax → Probabilities\n```\n\n### Key Components\n\n1. **Token Embedding (wte)**: Maps token IDs to vectors\n2. **N Transformer Blocks**: Process and transform representations\n3. **Language Model Head (lm_head)**: Maps final vectors to vocab logits\n4. **Untied Weights**: `wte` and `lm_head` are separate (not shared)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9ihwdat9qwh",
   "source": "class GPT(nn.Module):\n    \"\"\"\n    Simplified GPT model for demonstration\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        \n        # Token embeddings\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        \n        # Stack of transformer blocks\n        self.blocks = nn.ModuleList([\n            Block(config, layer_idx) \n            for layer_idx in range(config.n_layer)\n        ])\n        \n        # Language model head (output projection)\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        \n        # Precompute rotary embeddings\n        head_dim = config.n_embd // config.n_head\n        self.cos, self.sin = precompute_rotary_embeddings(config.sequence_len, head_dim)\n    \n    def forward(self, idx, targets=None):\n        B, T = idx.size()  # batch size, sequence length\n        \n        # Get rotary embeddings for this sequence\n        cos_sin = (self.cos[:, :T], self.sin[:, :T])\n        \n        # Step 1: Token embedding\n        x = self.wte(idx)  # (B, T, n_embd)\n        \n        # Step 2: Normalize embeddings (modern practice)\n        x = norm(x)\n        \n        # Step 3: Pass through all transformer blocks\n        for block in self.blocks:\n            x = block(x, cos_sin)\n        \n        # Step 4: Final normalization\n        x = norm(x)\n        \n        # Step 5: Project to vocabulary logits\n        logits = self.lm_head(x)  # (B, T, vocab_size)\n        \n        # If targets provided, compute loss\n        if targets is not None:\n            loss = F.cross_entropy(\n                logits.view(-1, logits.size(-1)), \n                targets.view(-1),\n                ignore_index=-1\n            )\n            return loss\n        else:\n            return logits\n\n# Create the complete model\nmodel = GPT(config)\n\nprint(\"=\"*60)\nprint(\"COMPLETE GPT MODEL\")\nprint(\"=\"*60)\nprint()\nprint(f\"Configuration:\")\nprint(f\"  Vocabulary size: {config.vocab_size:,}\")\nprint(f\"  Embedding dimension: {config.n_embd}\")\nprint(f\"  Number of layers: {config.n_layer}\")\nprint(f\"  Number of heads: {config.n_head}\")\nprint(f\"  Sequence length: {config.sequence_len}\")\nprint()\n\n# Count parameters\nn_params = sum(p.numel() for p in model.parameters())\nn_embed = model.wte.weight.numel()\nn_blocks = sum(p.numel() for p in model.blocks.parameters())\nn_head = model.lm_head.weight.numel()\n\nprint(f\"Parameters:\")\nprint(f\"  Token embedding: {n_embed:,}\")\nprint(f\"  Transformer blocks: {n_blocks:,}\")\nprint(f\"  LM head: {n_head:,}\")\nprint(f\"  Total: {n_params:,}\")\nprint()\nprint(f\"  Most parameters ({n_blocks/n_params*100:.1f}%) are in the transformer blocks!\")\nprint()\n\n# Test forward pass\nbatch_size = 2\nseq_len = 16\ntoken_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n\nprint(f\"Forward pass test:\")\nprint(f\"  Input shape: {token_ids.shape} (batch, seq_len)\")\n\nlogits = model(token_ids)\nprint(f\"  Output shape: {logits.shape} (batch, seq_len, vocab_size)\")\nprint()\nprint(f\"✓ Model successfully produces logits for each token position!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wg49msvpqj",
   "source": "## Section 8: Complete Forward Pass Trace\n\nLet's trace how data flows through the entire model for a single example.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "th6laiinv1a",
   "source": "# Trace a forward pass step by step\nprint(\"=\"*70)\nprint(\"FORWARD PASS TRACE\")\nprint(\"=\"*70)\nprint()\n\n# Create a small input (batch=1, seq_len=4 for clarity)\ninput_ids = torch.tensor([[42, 100, 256, 89]])  # Example token IDs\nB, T = input_ids.shape\n\nprint(f\"Input: {input_ids.tolist()[0]}\")\nprint(f\"  Shape: {input_ids.shape} (batch_size=1, seq_len=4)\")\nprint()\n\n# Manually trace through the forward pass\nprint(\"Step 1: Token Embedding\")\nx = model.wte(input_ids)\nprint(f\"  {input_ids.shape} → {x.shape}\")\nprint(f\"  Each token ID is mapped to a {config.n_embd}-dimensional vector\")\nprint(f\"  Example: token {input_ids[0, 0].item()} → vector of shape {x[0, 0].shape}\")\nprint()\n\nprint(\"Step 2: Initial RMSNorm\")\nx = norm(x)\nprint(f\"  {x.shape} (shape unchanged)\")\nprint(f\"  Normalizes embedding vectors for stable training\")\nprint()\n\n# Through each block\nfor layer_idx, block in enumerate(model.blocks):\n    print(f\"Step {3+layer_idx}: Transformer Block {layer_idx+1}\")\n    \n    x_before = x.clone()\n    cos_sin = (model.cos[:, :T], model.sin[:, :T])\n    x = block(x, cos_sin)\n    \n    # Check how much the block changed the representation\n    change_norm = (x - x_before).norm().item()\n    \n    print(f\"  Input:  {x_before.shape}\")\n    print(f\"  Output: {x.shape}\")\n    print(f\"  Change magnitude: {change_norm:.4f}\")\n    print(f\"  ├─ Attention: Each token attends to previous tokens\")\n    print(f\"  └─ MLP: Process gathered information\")\n    print()\n\nprint(f\"Step {3+config.n_layer}: Final RMSNorm\")\nx = norm(x)\nprint(f\"  {x.shape} (shape unchanged)\")\nprint()\n\nprint(f\"Step {4+config.n_layer}: Language Model Head\")\nlogits = model.lm_head(x)\nprint(f\"  {x.shape} → {logits.shape}\")\nprint(f\"  Projects each position's vector to vocabulary logits\")\nprint()\n\nprint(\"Step Final: Convert Logits to Probabilities (Softmax)\")\nprobs = F.softmax(logits, dim=-1)\nprint(f\"  {logits.shape} → {probs.shape}\")\nprint(f\"  Each position now has a probability distribution over {config.vocab_size} tokens\")\nprint()\n\n# Show predictions for the last position\nlast_pos_probs = probs[0, -1]  # Probabilities for next token after sequence\ntop5_probs, top5_indices = torch.topk(last_pos_probs, k=5)\n\nprint(\"Predictions for next token (after last position):\")\nprint(\"  Top 5 most likely tokens:\")\nfor i, (prob, idx) in enumerate(zip(top5_probs, top5_indices)):\n    print(f\"    {i+1}. Token {idx.item():3d}: {prob.item()*100:5.2f}%\")\nprint()\n\nprint(\"=\"*70)\nprint(\"Summary: The model transformed token IDs → meaningful representations\")\nprint(\"         → probability distributions for next token prediction!\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "tgt5forggnm",
   "source": "## Summary: Why This Architecture Works\n\n### Key Insights\n\n1. **Attention is All You Need**\n   - Self-attention lets each token gather information from all previous tokens\n   - Multi-head attention learns different types of relationships\n   - Causal masking ensures autoregressive generation\n\n2. **Positional Encoding Matters**\n   - RoPE encodes relative positions through rotation\n   - No learnable parameters needed\n   - Better generalization than absolute positions\n\n3. **Residual Connections Enable Deep Networks**\n   - Direct gradient flow prevents vanishing gradients\n   - Each layer makes small refinements\n   - Model can learn identity if needed\n\n4. **Normalization Stabilizes Training**\n   - RMSNorm keeps activations in reasonable range\n   - Pre-norm (normalize before, not after) is more stable\n   - QK norm in attention prevents training instabilities\n\n5. **MLP Provides Compute**\n   - Attention: \"What information to gather?\"\n   - MLP: \"How to process that information?\"\n   - 4x expansion gives model \"thinking room\"\n\n### Modern Improvements Over Original GPT\n\n| Feature | Original GPT-2 | This Implementation |\n|---------|---------------|---------------------|\n| Position Encoding | Learned embeddings | Rotary (RoPE) |\n| Normalization | LayerNorm | RMSNorm |\n| Attention Stability | None | QK Normalization |\n| Activation | GELU | ReLU² |\n| Weights | Tied wte/lm_head | Untied |\n| Bias | Yes | No |\n| KV Cache | Standard | GQA support |\n\n### Parameter Scaling\n\nFor a typical config:\n- **Embedding layer**: ~10% of parameters\n- **Transformer blocks**: ~85% of parameters\n  - Attention: ~30% of block params\n  - MLP: ~70% of block params\n- **LM head**: ~5% of parameters\n\n**Key insight**: Most computation is in the MLP layers!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "mfsxawo1ct8",
   "source": "## Visual Architecture Diagram\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                         GPT MODEL                            │\n└─────────────────────────────────────────────────────────────┘\n\nInput: Token IDs [42, 100, 256, 89]\n         │\n         ▼\n┌──────────────────────┐\n│  Token Embedding     │  vocab_size → n_embd\n│  (lookup table)      │  [42] → [0.1, -0.5, ..., 0.3]\n└──────────────────────┘\n         │\n         ▼\n┌──────────────────────┐\n│     RMSNorm          │  Normalize embeddings\n└──────────────────────┘\n         │\n         ▼\n┌──────────────────────────────────────────────┐\n│            TRANSFORMER BLOCK 1                │\n│  ┌─────────────────────────────────────┐     │\n│  │ x → Norm → Attention → (+) ← x      │     │\n│  │              ↓                       │     │\n│  │         Apply RoPE                   │     │\n│  │         QK Norm                      │     │\n│  │         Softmax(QK^T/√d)V           │     │\n│  └─────────────────────────────────────┘     │\n│              │                                │\n│  ┌───────────▼───────────────────────────┐   │\n│  │ x → Norm → MLP → (+) ← x              │   │\n│  │         Expand 4x                      │   │\n│  │         ReLU²                          │   │\n│  │         Project back                   │   │\n│  └────────────────────────────────────────┘   │\n└──────────────────────────────────────────────┘\n         │\n         ▼\n┌──────────────────────────────────────────────┐\n│         TRANSFORMER BLOCK 2...N               │\n│         (same structure)                      │\n└──────────────────────────────────────────────┘\n         │\n         ▼\n┌──────────────────────┐\n│     RMSNorm          │  Final normalization\n└──────────────────────┘\n         │\n         ▼\n┌──────────────────────┐\n│   Language Model     │  n_embd → vocab_size\n│      Head            │  vectors → logits\n└──────────────────────┘\n         │\n         ▼\n┌──────────────────────┐\n│     Softmax          │  logits → probabilities\n└──────────────────────┘\n         │\n         ▼\nOutput: Probabilities over vocabulary\n        [0.01, 0.001, ..., 0.15, ...]\n        \"Token 256 has 15% probability\"\n```\n\n---\n\n## Next Steps\n\nTo dive deeper:\n\n1. **Training**: See how the model learns from data (loss functions, backpropagation)\n2. **Generation**: Understand sampling strategies (greedy, top-k, top-p, temperature)\n3. **Optimization**: Learn about modern optimizers (AdamW, Muon)\n4. **Scaling Laws**: Understand how performance scales with model size and data\n5. **Fine-tuning**: Adapt pre-trained models to specific tasks\n\n## References\n\n- Original Transformer: \"Attention is All You Need\" (Vaswani et al., 2017)\n- GPT-2: \"Language Models are Unsupervised Multitask Learners\" (Radford et al., 2019)\n- RoPE: \"RoFormer: Enhanced Transformer with Rotary Position Embedding\" (Su et al., 2021)\n- GQA: \"GQA: Training Generalized Multi-Query Transformer\" (Ainslie et al., 2023)\n\n---\n\n## Practice Exercise\n\nTry modifying the config and observe changes:\n- Increase `n_layer`: Deeper network, more capacity\n- Increase `n_embd`: Wider network, more parameters\n- Change `n_kv_head < n_head`: Enable GQA\n- Increase `sequence_len`: Longer context window\n\nSee how parameter count and computation change!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "g9aerwn61et",
   "source": "# Quick Reference: Print model summary\nprint(\"=\"*70)\nprint(\"GPT ARCHITECTURE QUICK REFERENCE\")\nprint(\"=\"*70)\nprint()\n\nprint(\"Components:\")\nprint(\"  1. Token Embedding (wte): token_id → vector\")\nprint(\"  2. RMSNorm: x / sqrt(mean(x²))\")\nprint(\"  3. Rotary Embeddings (RoPE): Rotate Q,K by position\")\nprint(\"  4. Multi-Head Attention: Softmax(QK^T/√d)V\")\nprint(\"  5. QK Normalization: Normalize Q,K before attention\")\nprint(\"  6. MLP: Linear → ReLU² → Linear\")\nprint(\"  7. Residual: x = x + f(x)\")\nprint(\"  8. Language Model Head: vector → logits\")\nprint()\n\nprint(\"Data Flow:\")\nprint(\"  tokens → embed → norm → [attn+MLP blocks] → norm → lm_head → logits\")\nprint()\n\nprint(\"Key Formulas:\")\nprint(\"  • Attention: softmax(Q @ K^T / √d) @ V\")\nprint(\"  • RMSNorm: x / sqrt(mean(x²))\")\nprint(\"  • RoPE: rotate(x, angle=position*frequency)\")\nprint(\"  • ReLU²: max(0, x)²\")\nprint(\"  • Residual: x_out = x_in + transformation(x_in)\")\nprint()\n\nprint(\"Parameter Breakdown (for our config):\")\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Memory (fp32): {total_params * 4 / 1024**2:.1f} MB\")\nprint(f\"  Memory (bf16): {total_params * 2 / 1024**2:.1f} MB\")\nprint()\n\nprint(\"=\"*70)\nprint(\"You now understand the core GPT architecture!\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}